{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2023-06-04T16:36:59.556000-07:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.6\n",
      "IPython version      : 8.13.2\n",
      "\n",
      "Compiler    : GCC 11.3.0\n",
      "OS          : Linux\n",
      "Release     : 5.15.0-1026-nvidia\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 224\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgxuser/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask import dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import optuna\n",
    "import gc\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask   : 2023.3.2\n",
      "numpy  : 1.23.5\n",
      "optuna : 3.1.1\n",
      "logging: 0.5.1.2\n",
      "xgboost: 1.7.5\n",
      "pandas : 1.5.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA H100 80GB HBM3 (UUID: GPU-5c583ee7-8fb1-b26a-4bea-16e45d984a32)\n",
      "GPU 1: NVIDIA H100 80GB HBM3 (UUID: GPU-a6ab06f5-a6e2-18c1-9dec-0dcd29f44a46)\n",
      "GPU 2: NVIDIA H100 80GB HBM3 (UUID: GPU-bb8d5098-3c56-c48d-a0a3-fcdfcec6d3f5)\n",
      "GPU 3: NVIDIA H100 80GB HBM3 (UUID: GPU-cdbe686b-1611-999b-8e8d-a3c5f35b40c4)\n",
      "GPU 4: NVIDIA H100 80GB HBM3 (UUID: GPU-0df1cef0-fc95-cc88-b5f4-239889b3acba)\n",
      "GPU 5: NVIDIA H100 80GB HBM3 (UUID: GPU-9dca657c-dbe4-08b7-fe7c-5653f183f0b6)\n",
      "GPU 6: NVIDIA H100 80GB HBM3 (UUID: GPU-33389782-e2ad-5022-997d-cf470313879c)\n",
      "GPU 7: NVIDIA H100 80GB HBM3 (UUID: GPU-926eaa05-87fb-35e0-0a98-adb2ad41d0be)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-04 16:37:08,848 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-06-04 16:37:08,849 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-06-04 16:37:08,849 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-06-04 16:37:08,849 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-06-04 16:37:08,849 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-06-04 16:37:08,849 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-06-04 16:37:08,849 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-06-04 16:37:08,849 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-06-04 16:37:08,849 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-06-04 16:37:08,850 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-06-04 16:37:08,850 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-06-04 16:37:08,850 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-06-04 16:37:08,866 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-06-04 16:37:08,866 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-06-04 16:37:08,869 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-06-04 16:37:08,869 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n"
     ]
    }
   ],
   "source": [
    "cluster = LocalCUDACluster(n_workers=8)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun  4 16:37:17 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA H100 80G...  On   | 00000000:1B:00.0 Off |                    0 |\n",
      "| N/A   31C    P0   109W / 700W |    525MiB / 81559MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA H100 80G...  On   | 00000000:43:00.0 Off |                    0 |\n",
      "| N/A   30C    P0   109W / 700W |    525MiB / 81559MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA H100 80G...  On   | 00000000:52:00.0 Off |                    0 |\n",
      "| N/A   33C    P0   113W / 700W |    525MiB / 81559MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA H100 80G...  On   | 00000000:61:00.0 Off |                    0 |\n",
      "| N/A   34C    P0   113W / 700W |    525MiB / 81559MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA H100 80G...  On   | 00000000:9D:00.0 Off |                    0 |\n",
      "| N/A   34C    P0   113W / 700W |    525MiB / 81559MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA H100 80G...  On   | 00000000:C3:00.0 Off |                    0 |\n",
      "| N/A   35C    P0   114W / 700W |    525MiB / 81559MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA H100 80G...  On   | 00000000:D1:00.0 Off |                    0 |\n",
      "| N/A   37C    P0   112W / 700W |    525MiB / 81559MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA H100 80G...  On   | 00000000:DF:00.0 Off |                    0 |\n",
      "| N/A   37C    P0   111W / 700W |    525MiB / 81559MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3502184      C   /usr/bin/python3                  522MiB |\n",
      "|    1   N/A  N/A   3502185      C   /usr/bin/python3                  522MiB |\n",
      "|    2   N/A  N/A   3502191      C   /usr/bin/python3                  522MiB |\n",
      "|    3   N/A  N/A   3502195      C   /usr/bin/python3                  522MiB |\n",
      "|    4   N/A  N/A   3502194      C   /usr/bin/python3                  522MiB |\n",
      "|    5   N/A  N/A   3502199      C   /usr/bin/python3                  522MiB |\n",
      "|    6   N/A  N/A   3502202      C   /usr/bin/python3                  522MiB |\n",
      "|    7   N/A  N/A   3502203      C   /usr/bin/python3                  522MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fold 0\n",
      "Loading fold 1\n",
      "Loading fold 2\n",
      "Loading fold 3\n",
      "Loading fold 4\n",
      "CPU times: user 2.07 s, sys: 3.33 s, total: 5.4 s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_folds = []\n",
    "val_folds = []\n",
    "train_ys = []\n",
    "val_ys = []\n",
    "\n",
    "for i in range(5):\n",
    "    print(f'Loading fold {i}')\n",
    "    train_fold_d = delayed(pd.read_csv)(f'../input3/xgtrain_fold_{i}_l.csv.gz')\n",
    "    train_fold = dd.from_delayed(train_fold_d)\n",
    "    \n",
    "    val_fold_d = delayed(pd.read_csv)(f'../input3/xgval_fold_{i}_l.csv.gz')\n",
    "    val_fold = dd.from_delayed(val_fold_d)\n",
    "    \n",
    "    \n",
    "    train_y = train_fold['target']\n",
    "    train_fold = train_fold[train_fold.columns.difference(['target'])]\n",
    "    \n",
    "    val_y = val_fold['target']\n",
    "    val_fold = val_fold[val_fold.columns.difference(['target'])]\n",
    "    \n",
    "    train_folds.append(train_fold)\n",
    "    val_folds.append(val_fold)\n",
    "    \n",
    "    train_ys.append(train_y)\n",
    "    val_ys.append(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv.zip')\n",
    "\n",
    "shift = 200\n",
    "\n",
    "target0 = train['loss'].values\n",
    "target = np.log(target0+shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logregobj(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    con =2\n",
    "    x =preds-labels\n",
    "    grad =con*x / (np.abs(x)+con)\n",
    "    hess =con**2 / (np.abs(x)+con)**2\n",
    "    return grad, hess \n",
    "\n",
    "\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(preds), np.exp(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oof = np.zeros((target.shape[0],))\n",
    "\n",
    "num_round = 1000\n",
    "\n",
    "def objective(trial):\n",
    "        \n",
    "    params = {\n",
    "        #'objective': 'reg:squarederror', \n",
    "        'base_score':7.76,\n",
    "        'tree_method':'gpu_hist',  # 'gpu_hist','hist'\n",
    "        'lambda': trial.suggest_loguniform('lambda',1e-3,10.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha',1e-3,10.0),\n",
    "        'gamma': trial.suggest_loguniform('gamma',1e-3,10.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.3,1.0),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.4, 1.0),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001,0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 25),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1,300),\n",
    "        'eval_metric': trial.suggest_categorical('eval_metric',['rmse']),\n",
    "\n",
    "    }\n",
    "\n",
    "    kf = KFold(5, shuffle=True, random_state=137)\n",
    "\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(train,target)):\n",
    "        dtrain = xgb.dask.DaskDMatrix(client, train_folds[i].values, train_ys[i], enable_categorical=True)\n",
    "        dval = xgb.dask.DaskDMatrix(client, val_folds[i].values, val_ys[i], enable_categorical=True)\n",
    "        \n",
    "        output = xgb.dask.train(client, params, dtrain, num_boost_round=num_round, obj=logregobj)\n",
    "        booster = output['booster']  # booster is the trained model\n",
    "        booster.set_param({'predictor': 'gpu_predictor'})\n",
    "        predictions = xgb.dask.predict(client, booster, dval)\n",
    "        predictions = predictions.compute()\n",
    "        train_oof[val_index] = np.exp(predictions) - shift\n",
    "        del dtrain, dval, output\n",
    "        gc.collect()\n",
    "        gc.collect()\n",
    "\n",
    "    mae = mean_absolute_error(target0, train_oof)\n",
    "    \n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)  # Setup the root logger.\n",
    "logger.addHandler(logging.FileHandler(\"optuna_xgb_output_l_7.log\", mode=\"w\"))\n",
    "\n",
    "optuna.logging.enable_propagation()  # Propagate logs to the root logger.\n",
    "optuna.logging.disable_default_handler()  # Stop showing logs in sys.stderr.\n",
    "\n",
    "study = optuna.load_study(storage=\"sqlite:///xgb_optuna_allstate_l_7.db\", study_name=\"five_fold_optuna_xgb_l_7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 250 µs, sys: 357 µs, total: 607 µs\n",
      "Wall time: 469 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logger.info(\"Start optimization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3501664/481481954.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda',1e-3,10.0),\n",
      "/tmp/ipykernel_3501664/481481954.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha',1e-3,10.0),\n",
      "/tmp/ipykernel_3501664/481481954.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'gamma': trial.suggest_loguniform('gamma',1e-3,10.0),\n",
      "/tmp/ipykernel_3501664/481481954.py:14: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.3,1.0),\n",
      "/tmp/ipykernel_3501664/481481954.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.4, 1.0),\n",
      "/tmp/ipykernel_3501664/481481954.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.001,0.1),\n",
      "[16:39:04] task [xgboost.dask-6]:tcp://127.0.0.1:40943 got new rank 0\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "study.optimize(objective, n_trials=100)\n",
    "df = study.trials_dataframe(attrs=('number', 'value', 'params', 'state'))\n",
    "df.to_csv('optuna_xgb_output_l_7.csv', index=False)\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
