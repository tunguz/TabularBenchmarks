{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c09d551-d6b4-4b37-81f8-61ab30b01c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8029dd33-e40f-470e-92d6-19412b8d0290",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TabularDataset('../input/train.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e6c35fe-a474-4119-995a-e5a2a28e0f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f1db722-a76f-4b54-8191-9f01d030e4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240921_170604\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.8\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.6.0: Mon Jul 29 21:16:46 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "Memory Avail:       11.98 GB / 24.00 GB (49.9%)\n",
      "Disk Space Avail:   1345.40 GB / 1858.19 GB (72.4%)\n",
      "===================================================\n",
      "Presets specified: ['good_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2024-09-21 13:06:09,961\tINFO worker.py:1743 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"AutogluonModels/ag-20240921_170604/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Beginning AutoGluon training ... Time limit = 894s\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m AutoGluon will save models to \"AutogluonModels/ag-20240921_170604/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Train Data Rows:    53333\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Train Data Columns: 784\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Label Column:       label\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Problem Type:       multiclass\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Train Data Class Count: 10\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tAvailable Memory:                    11675.76 MB\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tTrain Data (Original)  Memory Usage: 319.01 MB (2.7% of available memory)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t\t\tNote: Converting 21 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tUseless Original Features (Count: 71): ['1x1', '1x2', '1x3', '1x4', '1x5', '1x6', '1x7', '1x8', '1x9', '1x10', '1x11', '1x12', '1x17', '1x18', '1x19', '1x20', '1x21', '1x22', '1x23', '1x24', '1x25', '1x26', '1x27', '1x28', '2x1', '2x2', '2x3', '2x4', '2x5', '2x25', '2x26', '2x27', '2x28', '3x1', '3x2', '3x3', '3x27', '3x28', '4x1', '4x2', '4x28', '5x1', '6x1', '6x2', '7x1', '8x1', '17x1', '18x1', '21x1', '24x1', '24x2', '24x28', '25x1', '25x2', '25x28', '26x1', '26x2', '26x28', '27x1', '27x2', '27x3', '27x27', '27x28', '28x1', '28x2', '28x3', '28x4', '28x25', '28x26', '28x27', '28x28']\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t\tThis is typically a feature which has the same value for all rows.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tUnused Original Features (Count: 80): ['1x14', '1x15', '1x16', '2x6', '2x7', '2x8', '2x9', '2x23', '2x24', '3x4', '3x5', '3x6', '3x26', '4x3', '4x26', '4x27', '5x2', '5x3', '5x27', '5x28', '6x3', '6x28', '7x28', '10x1', '10x2', '11x1', '11x28', '12x1', '12x2', '12x28', '13x1', '13x2', '13x28', '14x1', '14x2', '14x28', '15x1', '15x2', '15x28', '16x1', '16x2', '16x3', '17x2', '17x28', '18x2', '18x28', '19x1', '19x2', '19x28', '20x1', '20x2', '20x28', '21x2', '21x28', '22x1', '22x2', '22x28', '23x1', '23x2', '23x27', '23x28', '24x27', '25x3', '25x26', '25x27', '26x3', '26x4', '26x26', '26x27', '27x4', '27x23', '27x24', '27x25', '27x26', '28x5', '28x6', '28x7', '28x22', '28x23', '28x24']\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t\t('int', []) : 80 | ['1x14', '1x15', '1x16', '2x6', '2x7', ...]\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t\t('int', []) : 633 | ['1x13', '2x10', '2x11', '2x12', '2x13', ...]\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t\t('int', [])       : 632 | ['1x13', '2x10', '2x11', '2x12', '2x13', ...]\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t\t('int', ['bool']) :   1 | ['9x1']\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t1.6s = Fit runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t633 features in original data used to generate 633 features in processed data.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tTrain Data (Processed) Memory Usage: 257.21 MB (2.2% of available memory)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Data preprocessing and feature engineering runtime = 1.9s ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t'NN_TORCH': {},\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t'CAT': {},\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t'XGB': {},\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t'FASTAI': {},\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting 11 L1 models ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 594.9s of the 892.57s of remaining time.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 24.94% memory usage per fold, 49.87%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=24.94%)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t0.9833\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t128.23s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t0.55s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 464.85s of the 762.52s of remaining time.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 19.51% memory usage per fold, 78.03%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=19.51%)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t0.9824\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t378.74s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t19.81s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 82.62s of the 380.29s of remaining time.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 18.24% memory usage per fold, 72.94%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=18.24%)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t0.9617\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t69.69s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t1.59s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 11.34s of the 309.01s of remaining time.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 170 due to low time. Expected time usage reduced from 19.8s -> 11.3s...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t0.9637\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t7.1s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t3.17s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 0.69s of the 298.36s of remaining time.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tWarning: Model is expected to require 19.4s to train, which exceeds the maximum time limit of 0.7s, skipping model...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tTime limit exceeded... Skipping RandomForestEntr_BAG_L1.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 0.06s of the 297.73s of remaining time.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 19.91% memory usage per fold, 79.63%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=19.91%)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tTime limit exceeded... Skipping CatBoost_BAG_L1.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 297.34s of remaining time.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 0.4, 'LightGBMXT_BAG_L1': 0.4, 'RandomForestGini_BAG_L1': 0.2}\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t0.9855\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t0.5s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting 11 L2 models ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 296.82s of the 296.78s of remaining time.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 24.22% memory usage per fold, 48.43%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=24.22%)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t0.9845\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t144.56s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t0.64s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 150.92s of the 150.87s of remaining time.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 19.23% memory usage per fold, 76.93%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=19.23%)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t0.9843\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t124.72s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t2.62s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 24.3s of the 24.26s of remaining time.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 18.74% memory usage per fold, 74.96%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=18.74%)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t0.9861\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t22.56s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t0.4s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -0.1s of remaining time.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L2': 0.857, 'RandomForestGini_BAG_L1': 0.071, 'NeuralNetFastAI_BAG_L2': 0.071}\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t0.9862\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t0.93s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m AutoGluon training complete, total runtime = 895.63s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 285.0 rows/s (6667 batch size)\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting 1 L1 models ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t22.32s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting 1 L1 models ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t110.22s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting 1 L1 models ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t17.06s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t7.1s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t3.17s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 0.4, 'LightGBMXT_BAG_L1': 0.4, 'RandomForestGini_BAG_L1': 0.2}\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t0.5s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting 1 L2 models ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2_FULL ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tStopping at the best epoch learned earlier - 18.\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t20.72s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting 1 L2 models ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t337.59s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting 1 L2 models ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t2.93s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L2': 0.857, 'RandomForestGini_BAG_L1': 0.071, 'NeuralNetFastAI_BAG_L2': 0.071}\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m \t0.93s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Refit complete, total runtime = 512.82s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240921_170604/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=4850)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                          model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0        LightGBMXT_BAG_L1_FULL       0.982451   0.982450    accuracy        0.436514            NaN  110.216748                 0.436514                     NaN         110.216748            1       True          2\n",
      "1      WeightedEnsemble_L3_FULL       0.980801   0.986237    accuracy        0.813424            NaN  181.289789                 0.001203                     NaN           0.931634            3       True          9\n",
      "2      WeightedEnsemble_L2_FULL       0.979151   0.985506    accuracy        0.622497            NaN  140.138631                 0.001349                     NaN           0.504190            2       True          5\n",
      "3        LightGBMXT_BAG_L2_FULL       0.978851   0.984269    accuracy        0.811556            NaN  494.292125                 0.124399                     NaN         337.593555            2       True          7\n",
      "4   NeuralNetFastAI_BAG_L2_FULL       0.978551   0.984456    accuracy        0.781575            NaN  177.423328                 0.094418                     NaN          20.724758            2       True          6\n",
      "5          LightGBM_BAG_L2_FULL       0.976901   0.986144    accuracy        0.717803            NaN  159.633397                 0.030646                     NaN           2.934827            2       True          8\n",
      "6  RandomForestGini_BAG_L1_FULL       0.960252   0.963681    accuracy        0.097372       3.172225    7.101641                 0.097372                3.172225           7.101641            1       True          4\n",
      "7          LightGBM_BAG_L1_FULL       0.958452   0.961712    accuracy        0.066009            NaN   17.064129                 0.066009                     NaN          17.064129            1       True          3\n",
      "8   NeuralNetFastAI_BAG_L1_FULL       0.945703   0.983312    accuracy        0.087262            NaN   22.316052                 0.087262                     NaN          22.316052            1       True          1\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1416s\t = DyStack   runtime |\t2184s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 2184s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240921_170604\"\n",
      "Train Data Rows:    60000\n",
      "Train Data Columns: 784\n",
      "Label Column:       label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 10\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    12438.09 MB\n",
      "\tTrain Data (Original)  Memory Usage: 358.89 MB (2.9% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 17 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 67): ['1x1', '1x2', '1x3', '1x4', '1x5', '1x6', '1x7', '1x8', '1x9', '1x10', '1x11', '1x12', '1x17', '1x18', '1x19', '1x20', '1x21', '1x22', '1x23', '1x24', '1x25', '1x26', '1x27', '1x28', '2x1', '2x2', '2x3', '2x4', '2x25', '2x26', '2x27', '2x28', '3x1', '3x2', '3x27', '3x28', '4x1', '4x2', '4x28', '5x1', '6x1', '6x2', '7x1', '18x1', '21x1', '24x1', '24x2', '24x28', '25x1', '25x2', '25x28', '26x1', '26x2', '26x28', '27x1', '27x2', '27x3', '27x27', '27x28', '28x1', '28x2', '28x3', '28x4', '28x25', '28x26', '28x27', '28x28']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 93): ['1x14', '1x15', '1x16', '2x5', '2x6', '2x7', '2x8', '2x9', '2x10', '2x15', '2x16', '2x17', '2x18', '2x19', '2x23', '2x24', '3x3', '3x4', '3x5', '3x26', '4x4', '4x5', '4x26', '4x27', '5x2', '5x27', '5x28', '6x28', '7x28', '8x1', '8x28', '9x1', '9x28', '10x1', '10x28', '11x1', '11x28', '12x1', '12x28', '13x1', '13x28', '14x1', '14x2', '14x28', '15x1', '15x2', '15x28', '16x1', '16x2', '16x3', '16x28', '17x1', '17x2', '17x3', '17x28', '18x2', '18x28', '19x1', '19x2', '19x27', '19x28', '20x1', '20x2', '20x27', '20x28', '21x27', '21x28', '22x1', '22x28', '23x1', '23x2', '23x27', '23x28', '24x27', '25x3', '25x26', '25x27', '26x3', '26x4', '26x5', '26x25', '26x26', '26x27', '27x4', '27x5', '27x25', '27x26', '28x5', '28x6', '28x21', '28x22', '28x23', '28x24']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 93 | ['1x14', '1x15', '1x16', '2x5', '2x6', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 624 | ['1x13', '2x11', '2x12', '2x13', '2x14', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 624 | ['1x13', '2x11', '2x12', '2x13', '2x14', ...]\n",
      "\t1.6s = Fit runtime\n",
      "\t624 features in original data used to generate 624 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 285.64 MB (2.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.91s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1454.06s of the 2181.63s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 24.58% memory usage per fold, 49.15%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=24.58%)\n",
      "\t0.9833\t = Validation score   (accuracy)\n",
      "\t1143.98s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 308.13s of the 1035.7s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 19.84% memory usage per fold, 79.34%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=19.84%)\n",
      "\t0.9654\t = Validation score   (accuracy)\n",
      "\t1103.71s\t = Training   runtime\n",
      "\t5.28s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -70.89s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.533, 'NeuralNetFastAI_BAG_L1': 0.467}\n",
      "\t0.9841\t = Validation score   (accuracy)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 11 L2 models ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -71.51s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.533, 'NeuralNetFastAI_BAG_L1': 0.467}\n",
      "\t0.9841\t = Validation score   (accuracy)\n",
      "\t0.32s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2255.49s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1245.5 rows/s (7500 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "No improvement since epoch 0: early stopping\n",
      "\t22.04s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\t36.07s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.533, 'NeuralNetFastAI_BAG_L1': 0.467}\n",
      "\t0.47s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.533, 'NeuralNetFastAI_BAG_L1': 0.467}\n",
      "\t0.32s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 58.71s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240921_170604\")\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "predictor = TabularPredictor(label='label').fit(train_data, presets='good_quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5338991-d117-47b1-a9f9-be4e4a7fb548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: ../input/test.csv.gz | Columns = 785 / 785 | Rows = 10000 -> 10000\n"
     ]
    }
   ],
   "source": [
    "test_data = TabularDataset('../input/test.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3e33589-68b1-4101-b726-cde83549b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6e2c4fb-b780-4b0d-a0ca-decd39eb8d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       7\n",
       "1       2\n",
       "2       1\n",
       "3       0\n",
       "4       4\n",
       "       ..\n",
       "9995    2\n",
       "9996    3\n",
       "9997    4\n",
       "9998    5\n",
       "9999    6\n",
       "Name: label, Length: 10000, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af35bec3-4e8a-430b-9761-c328cfd488de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       7\n",
       "1       2\n",
       "2       1\n",
       "3       0\n",
       "4       4\n",
       "       ..\n",
       "9995    2\n",
       "9996    3\n",
       "9997    4\n",
       "9998    5\n",
       "9999    6\n",
       "Name: label, Length: 10000, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4abf6583-343e-47bd-9526-2acd5ad8ccaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9735,\n",
       " 'balanced_accuracy': 0.9733296531482674,\n",
       " 'mcc': 0.9705466444986919}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(test_data, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8683adb-6850-4b4f-a102-7c47c8460213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBMXT_BAG_L1_FULL</td>\n",
       "      <td>0.9777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.189611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.066360</td>\n",
       "      <td>0.189611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.066360</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L3_FULL</td>\n",
       "      <td>0.9735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.295471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.427785</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.320332</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WeightedEnsemble_L2_FULL</td>\n",
       "      <td>0.9735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.296606</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.579768</td>\n",
       "      <td>0.001975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.472315</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NeuralNetFastAI_BAG_L1_FULL</td>\n",
       "      <td>0.9545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.105020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.041093</td>\n",
       "      <td>0.105020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.041093</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WeightedEnsemble_L3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.98415</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.026092</td>\n",
       "      <td>2248.018348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004806</td>\n",
       "      <td>0.320332</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.98415</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.026097</td>\n",
       "      <td>2248.170331</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004811</td>\n",
       "      <td>0.472315</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NeuralNetFastAI_BAG_L1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.98335</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.745283</td>\n",
       "      <td>1143.984700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.745283</td>\n",
       "      <td>1143.984700</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LightGBMXT_BAG_L1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.96535</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.276003</td>\n",
       "      <td>1103.713316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.276003</td>\n",
       "      <td>1103.713316</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model  score_test  score_val eval_metric  \\\n",
       "0       LightGBMXT_BAG_L1_FULL      0.9777        NaN    accuracy   \n",
       "1     WeightedEnsemble_L3_FULL      0.9735        NaN    accuracy   \n",
       "2     WeightedEnsemble_L2_FULL      0.9735        NaN    accuracy   \n",
       "3  NeuralNetFastAI_BAG_L1_FULL      0.9545        NaN    accuracy   \n",
       "4          WeightedEnsemble_L3         NaN    0.98415    accuracy   \n",
       "5          WeightedEnsemble_L2         NaN    0.98415    accuracy   \n",
       "6       NeuralNetFastAI_BAG_L1         NaN    0.98335    accuracy   \n",
       "7            LightGBMXT_BAG_L1         NaN    0.96535    accuracy   \n",
       "\n",
       "   pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  \\\n",
       "0        0.189611            NaN    36.066360                 0.189611   \n",
       "1        0.295471            NaN    58.427785                 0.000840   \n",
       "2        0.296606            NaN    58.579768                 0.001975   \n",
       "3        0.105020            NaN    22.041093                 0.105020   \n",
       "4             NaN       6.026092  2248.018348                      NaN   \n",
       "5             NaN       6.026097  2248.170331                      NaN   \n",
       "6             NaN       0.745283  1143.984700                      NaN   \n",
       "7             NaN       5.276003  1103.713316                      NaN   \n",
       "\n",
       "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                     NaN          36.066360            1       True   \n",
       "1                     NaN           0.320332            3       True   \n",
       "2                     NaN           0.472315            2       True   \n",
       "3                     NaN          22.041093            1       True   \n",
       "4                0.004806           0.320332            3      False   \n",
       "5                0.004811           0.472315            2      False   \n",
       "6                0.745283        1143.984700            1      False   \n",
       "7                5.276003        1103.713316            1      False   \n",
       "\n",
       "   fit_order  \n",
       "0          6  \n",
       "1          8  \n",
       "2          7  \n",
       "3          5  \n",
       "4          4  \n",
       "5          3  \n",
       "6          1  \n",
       "7          2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdce03f-ffe0-429e-990a-4fdb51529d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
